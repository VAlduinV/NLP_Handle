Оптимальні гіперпараметри LSTM для корпусу «Лис Микита»

На основі корпусу обсягом ~700 KB (літературний текст) та за наявності 32 ГБ RAM і GPU 16 ГБ, доцільно використовувати модель генерації тексту на рівні символів (char-level LSTM) з такими налаштуваннями гіперпараметрів:

seq_len (довжина вхідної послідовності) – 100 символів. Це типова довжина контексту для символьних моделей, яка дозволяє захопити кілька слів або рядків тексту, зберігаючи контекст фрази або рими
karpathy.github.io
. Наприклад, у прикладах з художніми текстами (Шекспір та ін.) використовують послідовності саме ~100 символів
tensorflow.org
 – цього достатньо, щоб модель навчилася структури речень і діалогів, але не надто багато, щоб ускладнити навчання або спричинити просте запам’ятовування тренувального тексту.

batch_size (розмір міні-пакету) – 64. Такий розмір пакета є збалансованим для тренування на GPU: він досить великий для ефективного використання GPU-пам’яті, але не надто великий, щоб погіршити збіжність моделі
tensorflow.org
. У прикладах Keras для генерації тексту саме 64 взято як стандартний batch size
tensorflow.org
. Досвід показує, що зменшення batch size до ~64 (від, скажімо, 128) може покращити навчання на більшій кількості епох, вводячи більше стохастичності та запобігаючи перенавчанню
machinelearningmastery.com
, при цьому 32 ГБ ОЗП достатньо для комфортної роботи з таким пакетом.

rnn_units (кількість нейронів LSTM) – 1024. Велика кількість блоків LSTM (порядку тисячі) дозволить моделі вивчити складні залежності в літературному тексті (ритміку, пунктуацію, структуру речень) і зберігати довготривалу пам’ять про контекст
tensorflow.org
. З огляду на обсяг GPU 16 ГБ, модель з 1024 нейронами цілком поміститься в пам’ять і була успішно застосована в аналогічних задачах (наприклад, для творів Шекспіра використовували ~1024 LSTM-нейронів)
tensorflow.org
. Менша модель (наприклад, 256–512) може не охопити всі нюанси художнього стилю, тоді як 1024 забезпечує високу якість генерованого тексту.

embed_dim (розмірність векторного подання символів) – 256. Кожен символ буде відображено в простір розмірності 256, що дає моделі можливість навчитися корисним ознакам символів (наприклад, подібність літер за звучанням чи функцією)
tensorflow.org
. Значення 256 використано в офіційному прикладі TensorFlow для символьної моделі
tensorflow.org
, і воно добре працює для української абетки з урахуванням регістру та розділових знаків (близько кількох десятків унікальних символів). Така розмірність достатня, щоб модель не перевчилася на рівні окремих символів, але й не надто мала, щоб втрачати інформацію про них.

epochs (кількість епох навчання) – 50. Зважаючи на відносно невеликий корпус, моделі варто зробити кілька десятків проходів по даних, щоб максимально вивчити стилістику тексту. Практика показує, що після 20 епох якість все ще зростає, і мережа не перенавчається занадто швидко
machinelearningmastery.com
. Наприклад, у одному з експериментів втрати моделі зменшувались майже кожної епохи, тому збільшення кількості епох з 20 до 50 покращило результат
machinelearningmastery.com
. 50 епох на даному апаратному забезпеченні цілком реальні за часом; модель за цей час навчиться тонкощів мови твору (старовинні вислови, римування тощо). При потребі можна збільшити до 100, але слід слідкувати, щоб модель не почала дослівно відтворювати текст корпусу.

temperature (температура генерації) – 0.8. Така температура забезпечує баланс між випадковістю і достовірністю генерованого тексту. Трохи менше 1.0 означає, що модель буде обережніше обирати наступні символи (ймовірніші символи отримають ще більшу перевагу), тому текст виходить зв’язнішим і граматично правильнішим, що важливо для літературної мови. Водночас 0.8 все ще достатньо для творчості – модель не буде надто детерміновано повторювати найймовірніший варіант
codefinity.com
. (Для художніх задач рекомендують температуру в діапазоні ~0.8–1.0, де нижчі значення дають більш осмислений, хоч і менш різноманітний текст
codefinity.com
.)

top_k / top_p (обмеження вибору символів, опційно) – top_k = 10, top_p = 0.9. Ці параметри ввімкнені за бажанням для «керованої» генерації: top-k = 10 означає, що на кожному кроці модель випадково вибирає наступний символ лише з топ-10 найбільш ймовірних варіантів, відкидаючи всі інші
codefinity.com
. Top-p = 0.9 (nucleus sampling) обмежує вибір динамічно: враховуються лише ті символи, сумарна ймовірність яких досягає 90%
codefinity.com
. Обидва підходи запобігають вибору моделью дуже малоймовірних, «випадкових» символів, що підвищує зв’язність і стилістичну єдність тексту. Для даного завдання (літературна казка) це допоможе зберегти осмисленість: top_k = 10 забезпечує контрольовану різноманітність лексики, а top_p = 0.9 – гнучке обмеження, щоб текст залишався правдоподібним. (Можна використовувати один із параметрів або їх комбінацію залежно від бажаного ефекту.)

length (довжина згенерованого тексту) – ≈500 символів. Рекомендується генерувати фрагменти порядку кількох сотень символів для оцінки якості. Приблизно 500 символів (це кілька абзаців або одна сцена діалогу) достатньо, щоб побачити стилістику й контекст, але не надто довго, щоб модель «загубила» початковий контекст або почала повторюватися. Досвід показує, що якщо генерувати дуже довгі уривки (понад 1000 символів) без додаткового контролю, модель може зациклитися чи повторювати одні й ті самі фрази
machinelearningmastery.com
. Тому ~500 символів – оптимально для демонстрації: текст виходить зв’язним і якісним, його зручно читати і аналізувати на відповідність оригінальному стилю.

Джерела: Під час вибору гіперпараметрів враховано рекомендації та приклади з відкритих матеріалів (блоги, навчальні посібники) щодо генерації тексту LSTM-моделями
karpathy.github.io
tensorflow.org
machinelearningmastery.com
, а також специфіку самого корпусу «Лис Микита» (віршована казка з римами і діалогами). Наведені значення спрямовані на отримання максимально природного та стилістично близького до оригіналу тексту, використовуючи наявні ресурси апаратного забезпечення оптимально.